{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "from Assistant import AssistantOpenAI\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from git import Repo\n",
    "\n",
    "import numpy as np\n",
    "#from collections import deque, namedtuple\n",
    "\n",
    "# For visualization\n",
    "import gymnasium.wrappers.record_video as record_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 200\n",
    "ENV_NAME = 'LunarLander-v2'\n",
    "ARCLABKEY_OPENAI = \"sk-proj-DvHDR3hpgbm2r3kCA9jKT3BlbkFJL57ABXkfaWAIYKBxdhM6\"\n",
    "ARCLABKEY_OPENAI = \"sk-proj-GvaDXazpibWA2M1I5Pu2T3BlbkFJxDuKlr9AcoVG98ctJZ7Q\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function calling.\n",
    "\n",
    "Función encargada de almacenar el código generado mediante la opción de function calling del asistente de OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_code_in_file(code, filename):\n",
    "    \"\"\" Store code in a file\n",
    "    \n",
    "    Args:\n",
    "        code: str: code to store\n",
    "        filename: str: filename to store code in\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(code)\n",
    "        \n",
    "store_code_in_file_schema = {\n",
    "    \"name\": \"store_code_in_file\",\n",
    "    \"description\": \"Store code in a file\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"code\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The Python code to store.\"\n",
    "            },\n",
    "            \"filename\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The filename to store the code in.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"code\", \"filename\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "TOOLS = [{\"type\": \"function\", \"function\": store_code_in_file_schema}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entorno Lunnar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_initial_code = \"\"\"\n",
    "import random\n",
    "\n",
    "def act(observation):\n",
    "    '''\n",
    "    The function that codifies the action to be taken in each instant of time.\n",
    "\n",
    "    Args:\n",
    "        observation (numpy.array):\n",
    "            \"description\": \"The state of the environment after the action is taken.\",\n",
    "            \"positions\": {  \n",
    "                \"0\": \"X position\",\n",
    "                \"1\": \"Y position\",\n",
    "                \"2\": \"X velocity\",\n",
    "                \"3\": \"Y velocity\",\n",
    "                \"4\": \"Angle\",\n",
    "                \"5\": \"Angular velocity\",\n",
    "                \"6\": \"Left contact sensor\",\n",
    "                \"7\": \"Right contact sensor\"\n",
    "            },\n",
    "            \"min_values\": [-1.5, -1.5, -5.0, -5.0, -3.14, -5.0, 0, 0],\n",
    "            \"max_values\": [1.5, 1.5, 5.0, 5.0, 3.14, 5.0, 1, 1]\n",
    "\n",
    "    Returns:\n",
    "        Integer  : The action to be taken.\n",
    "        \"options\": {\n",
    "                '0' : \"Switch off engines\",\n",
    "                '1' : \"Push left engine\",\n",
    "                '2' : \"Push both engines (upwards)\",\n",
    "                '3' : \"Push right engine\"\n",
    "            }\n",
    "    '''\n",
    "    return random.randint(0, 3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapeador de logs a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_description = {\n",
    "    \"description\": \"Log data for each step of the spacecraft landing environment.\",\n",
    "    \"landing attempt\": {\n",
    "        \"type\": \"integer\",\n",
    "        \"description\": \"The episode number.\"\n",
    "    },\n",
    "    \"logs\": {\n",
    "        \"instant\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"The instant within the landing attempt where the current log is taken.\"\n",
    "        },\n",
    "        \"action\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"The possible actions taken.\",\n",
    "            \"options\": {\n",
    "                '0' : \"Switch off engines\",\n",
    "                '1' : \"Push left engine\",\n",
    "                '2' : \"Push both engines (upwards)\",\n",
    "                '3' : \"Push right engine\"\n",
    "            }\n",
    "        },  \n",
    "        \"current status\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"The state of the environment after the action is taken.\",\n",
    "            \"positions\": {  \n",
    "                \"0\": \"X position\",\n",
    "                \"1\": \"Y position\",\n",
    "                \"2\": \"X velocity\",\n",
    "                \"3\": \"Y velocity\",\n",
    "                \"4\": \"Angle\",\n",
    "                \"5\": \"Angular velocity\",\n",
    "                \"6\": \"Left contact sensor with landing zone\",\n",
    "                \"7\": \"Right contact sensor with landing zone\"\n",
    "            },\n",
    "            \"min_values\": [-1.5, -1.5, -5.0, -5.0, -3.14, -5.0, 0, 0],\n",
    "            \"max_values\": [1.5, 1.5, 5.0, 5.0, 3.14, 5.0, 1, 1]\n",
    "        }, \n",
    "        \"score\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"The score received for the action.\"\n",
    "        },  \n",
    "        \"completed\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether the landing event has ended (landing or accident).\"\n",
    "        }\n",
    "    },\n",
    "    \"total score\": {\n",
    "        \"type\": \"number\",\n",
    "        \"description\": \"The total score received for the landing attempt.\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def format_step_data(instant, action, next_state, reward, terminated):\n",
    "    \"\"\" Format the step data into a JSON string\n",
    "    \n",
    "    Args:\n",
    "        instant: int: the instant within the landing attempt where the current log is taken\n",
    "        action: int: the action taken\n",
    "        next_state: np.array: the next state of the environment\n",
    "        reward: float: the reward received\n",
    "        terminated: bool: whether the landing event has ended\n",
    "        \n",
    "    Returns:\n",
    "        str: the step data formatted as a JSON string\n",
    "    \"\"\"\n",
    "    # Convertir el array numpy a una lista\n",
    "    next_state_list = next_state.tolist()\n",
    "\n",
    "    # Redondear los elementos de la lista a 4 decimales\n",
    "    next_state_list_rounded = [round(x, 3) for x in next_state_list]\n",
    "\n",
    "    step_data = {\n",
    "        'instant': instant,\n",
    "        'action' : int(action),\n",
    "        'current status': next_state_list_rounded,  # Convert numpy array to list\n",
    "        'score': round(reward, 3),\n",
    "        'completed': terminated,\n",
    "        #'truncated': truncated\n",
    "        #'info': info\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a JSON string\n",
    "    step_data_json = json.dumps(step_data)\n",
    "\n",
    "    return step_data_json\n",
    "\n",
    "\n",
    "def format_episode_logs(logs, episode, total_score):\n",
    "    \"\"\" Format the logs into a JSON string\n",
    "    \n",
    "    Args:\n",
    "        logs: list: the logs for each step of the environment\n",
    "        episode: int: the episode number\n",
    "        \n",
    "    Returns:\n",
    "        str: the logs formatted as a JSON string\n",
    "    \"\"\"\n",
    "    logs_data = {\n",
    "        'landing attempt': episode,\n",
    "        'logs': logs,\n",
    "        'total score': total_score\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a JSON string\n",
    "    logs_json = json.dumps(logs_data)\n",
    "\n",
    "    return logs_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commit_changes(repo_path, commit_message):\n",
    "    \"\"\" Commit changes to the repository.\n",
    "\n",
    "    Args:\n",
    "        repo_path (str): Path to the repository.\n",
    "        commit_message (str): The commit message.\n",
    "    \"\"\"\n",
    "    repo = Repo(repo_path)\n",
    "    repo.git.add(update=True)\n",
    "    repo.index.commit(commit_message)\n",
    "    origin = repo.remote(name='origin')\n",
    "    origin.push()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Código encargado de ejecutar los eventos en el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Action\n",
    "\n",
    "def lunar_lander(max_t=1000, n_games=1, display=False, seed=38, agent=None, recoder=False, video_filename='video/video.mp4'):\n",
    "    \"\"\" Run the Lunar Lander environment\n",
    "    \n",
    "    Args:\n",
    "        max_t: int: the maximum number of timesteps\n",
    "        n_games: int: the number of games to play\n",
    "        display: bool: whether to display the environment\n",
    "        seed: int: the seed for the environment\n",
    "        agent: object: the agent to use\n",
    "        recoder: bool: whether to record the video\n",
    "        video_filename: str: the filename for the video\n",
    "        \n",
    "    Returns:\n",
    "        str: the logs for the landing attempts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Incluimos la opción del entorno gráfico y la de grabación.\n",
    "    if display:\n",
    "        env = gym.make(ENV_NAME, render_mode='human')\n",
    "        if recoder:\n",
    "            env = gym.make(ENV_NAME, render_mode='rgb_array')\n",
    "            env.reset() \n",
    "            video_recorder = record_video.RecordVideo(env, video_filename)\n",
    "    else:\n",
    "        env = gym.make(ENV_NAME)\n",
    "\n",
    "    # Bucle principal de ejecución de los episodios.\n",
    "    logs = []\n",
    "    for episode in range(1, n_games+1):\n",
    "        state = env.reset(seed=seed) # Set a seed for the environment\n",
    "        state = state[0] # Eliminamos el diccionario vacio y dejamos unicamente el estado de 8 elementos.\n",
    "        score = 0\n",
    "        instant = 0\n",
    "        episode_actions = []\n",
    "        if recoder: # En caso de que se quiera grabar el video, se inicia el grabador.\n",
    "            video_recorder.start_video_recorder()\n",
    "            \n",
    "        for i in range(max_t): # Bucle de ejecución de los instantes de tiempo.\n",
    "            \n",
    "            # Seleccionamos el método de elegir la acción (agente exitoso o método de decisión del asistente)\n",
    "            if agent: \n",
    "                action = agent.act(state)\n",
    "            else:\n",
    "                action = Action.act(state)\n",
    "            \n",
    "            # Avanzamos un instante de tiempo en el entorno en función de si se quiere grabar el video o no.\n",
    "            if recoder:\n",
    "                next_state, reward, terminated, truncated, info = video_recorder.step(action)\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            # Formateamos los datos del instante de tiempo y los alcenamos.\n",
    "            json_logs = format_step_data(instant, action, next_state, reward, terminated)\n",
    "            score += reward\n",
    "            instant += 1\n",
    "            if len(episode_actions) == 0 or len(episode_actions) % 2 == 0 or terminated:\n",
    "                logs.append(json_logs)\n",
    "            episode_actions.append(action)\n",
    "            state = next_state\n",
    "            if terminated: # Condición de salida del bucle, si el episodio ha terminado.\n",
    "                break\n",
    "        json_episode_logs = format_episode_logs(logs, episode, score) \n",
    "        \n",
    "        print(f\"Número de instantes: {instant+1}. Tamaño de logs: {len(logs)}\")\n",
    "        print('episode ', episode, 'score %.3f' % float(score), 'avg score %.3f' % (float(score) / instant))\n",
    "        \n",
    "    if recoder:\n",
    "        video_recorder.close()\n",
    "    else:\n",
    "        env.close()\n",
    "        \n",
    "    return json_episode_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs del código inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de instantes: 73. Tamaño de logs: 37\n",
      "episode  1 score -612.950 avg score -8.513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"landing attempt\": 1, \"logs\": [\"{\\\\\"instant\\\\\": 0, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.007, 1.417, 0.37, 0.131, -0.007, -0.044, 0.0, 0.0], \\\\\"score\\\\\": 1.387, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 2, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.015, 1.421, 0.353, 0.077, -0.006, 0.026, 0.0, 0.0], \\\\\"score\\\\\": 1.473, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 4, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.021, 1.423, 0.332, 0.024, 0.003, 0.11, 0.0, 0.0], \\\\\"score\\\\\": 1.038, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 6, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.028, 1.422, 0.312, -0.03, 0.02, 0.189, 0.0, 0.0], \\\\\"score\\\\\": 0.039, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 8, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.034, 1.419, 0.29, -0.084, 0.046, 0.277, 0.0, 0.0], \\\\\"score\\\\\": -0.807, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 10, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.04, 1.414, 0.272, -0.138, 0.079, 0.349, 0.0, 0.0], \\\\\"score\\\\\": -1.802, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 12, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.046, 1.406, 0.254, -0.192, 0.119, 0.422, 0.0, 0.0], \\\\\"score\\\\\": -2.398, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 14, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.051, 1.395, 0.235, -0.248, 0.166, 0.496, 0.0, 0.0], \\\\\"score\\\\\": -3.172, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 16, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.056, 1.382, 0.215, -0.304, 0.222, 0.578, 0.0, 0.0], \\\\\"score\\\\\": -3.857, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 18, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.06, 1.367, 0.194, -0.362, 0.287, 0.67, 0.0, 0.0], \\\\\"score\\\\\": -4.56, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 20, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.065, 1.349, 0.173, -0.42, 0.36, 0.758, 0.0, 0.0], \\\\\"score\\\\\": -5.145, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 22, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.069, 1.329, 0.154, -0.478, 0.442, 0.839, 0.0, 0.0], \\\\\"score\\\\\": -5.65, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 24, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.072, 1.306, 0.139, -0.538, 0.532, 0.916, 0.0, 0.0], \\\\\"score\\\\\": -6.215, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 26, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.076, 1.28, 0.123, -0.599, 0.63, 1.001, 0.0, 0.0], \\\\\"score\\\\\": -6.534, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 28, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.079, 1.252, 0.106, -0.663, 0.737, 1.099, 0.0, 0.0], \\\\\"score\\\\\": -7.093, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 30, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.081, 1.222, 0.093, -0.726, 0.854, 1.188, 0.0, 0.0], \\\\\"score\\\\\": -7.423, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 32, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.084, 1.188, 0.08, -0.791, 0.98, 1.289, 0.0, 0.0], \\\\\"score\\\\\": -8.016, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 34, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.086, 1.152, 0.071, -0.857, 1.117, 1.386, 0.0, 0.0], \\\\\"score\\\\\": -8.276, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 36, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.088, 1.113, 0.065, -0.922, 1.262, 1.476, 0.0, 0.0], \\\\\"score\\\\\": -8.699, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 38, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.09, 1.072, 0.062, -0.988, 1.416, 1.568, 0.0, 0.0], \\\\\"score\\\\\": -9.095, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 40, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.091, 1.028, 0.061, -1.055, 1.581, 1.673, 0.0, 0.0], \\\\\"score\\\\\": -9.416, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 42, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.092, 0.981, 0.063, -1.121, 1.755, 1.763, 0.0, 0.0], \\\\\"score\\\\\": -9.857, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 44, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.093, 0.931, 0.069, -1.184, 1.937, 1.842, 0.0, 0.0], \\\\\"score\\\\\": -9.938, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 46, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.093, 0.878, 0.08, -1.251, 2.13, 1.946, 0.0, 0.0], \\\\\"score\\\\\": -10.358, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 48, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.094, 0.822, 0.091, -1.313, 2.33, 2.025, 0.0, 0.0], \\\\\"score\\\\\": -10.513, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 50, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.094, 0.763, 0.108, -1.374, 2.539, 2.115, 0.0, 0.0], \\\\\"score\\\\\": -10.843, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 52, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.095, 0.701, 0.13, -1.434, 2.757, 2.209, 0.0, 0.0], \\\\\"score\\\\\": -10.925, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 54, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.096, 0.635, 0.148, -1.49, 2.984, 2.284, 0.0, 0.0], \\\\\"score\\\\\": -11.009, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 56, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.097, 0.566, 0.169, -1.543, 3.218, 2.359, 0.0, 0.0], \\\\\"score\\\\\": -11.137, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 58, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.098, 0.495, 0.192, -1.592, 3.461, 2.45, 0.0, 0.0], \\\\\"score\\\\\": -11.209, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 60, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.1, 0.42, 0.209, -1.637, 3.711, 2.529, 0.0, 0.0], \\\\\"score\\\\\": -11.288, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 62, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.102, 0.342, 0.223, -1.681, 3.971, 2.608, 0.0, 0.0], \\\\\"score\\\\\": -11.594, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 64, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.105, 0.262, 0.23, -1.722, 4.237, 2.685, 0.0, 0.0], \\\\\"score\\\\\": -11.824, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 66, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.109, 0.179, 0.234, -1.762, 4.512, 2.772, 0.0, 0.0], \\\\\"score\\\\\": -12.313, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 68, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.113, 0.094, 0.231, -1.802, 4.796, 2.863, 0.0, 0.0], \\\\\"score\\\\\": -13.326, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 70, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.118, 0.008, 0.227, -1.826, 5.08, 2.809, 1.0, 0.0], \\\\\"score\\\\\": -13.609, \\\\\"completed\\\\\": false}\", \"{\\\\\"instant\\\\\": 71, \\\\\"action\\\\\": 1, \\\\\"current status\\\\\": [0.122, -0.003, 0.469, -0.555, 4.795, -5.984, 1.0, 0.0], \\\\\"score\\\\\": -100, \\\\\"completed\\\\\": true}\"], \"total score\": -612.9504793446796}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_logs = lunar_lander(n_games=1, display=True)\n",
    "initial_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs exitosos\n",
    "Utilizar otras semillas para que no memorice el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mDuelingDQN\u001b[39;00m\n\u001b[0;32m      3\u001b[0m agent \u001b[38;5;241m=\u001b[39m DuelingDQN\u001b[38;5;241m.\u001b[39mAgent(num_observaciones\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, num_acciones\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, red_modelo\u001b[38;5;241m=\u001b[39mDuelingDQN\u001b[38;5;241m.\u001b[39mDuelingQNetwork, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Cargamos los pesos del agente entrenado.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adria\\Repos\\TFM__LLM_landing_self-refinement\\DuelingDQN.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:1541\u001b[0m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;66;03m# nn.quant* depends on ao -- so should be after those.\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizable\u001b[39;00m\n\u001b[1;32m-> 1541\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantized\u001b[39;00m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqat\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintrinsic\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import DuelingDQN\n",
    "\n",
    "agent = DuelingDQN.Agent(num_observaciones=8, num_acciones=4, red_modelo=DuelingDQN.DuelingQNetwork, seed=0)\n",
    "# Cargamos los pesos del agente entrenado.\n",
    "agent.load_weights('checkpoint_Dueling.pth')\n",
    "\n",
    "success_logs = lunar_lander(n_games=1, display=True, seed=42, agent=agent)\n",
    "success_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucle iterativo\n",
    "\n",
    "Sección principal del código encargada de conectar con el asistente de la API de OpenAI e iterar en la generación de código nuevo a partir de los registros del generado previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback, importlib\n",
    "import Action\n",
    "\n",
    "Action = importlib.reload(Action)\n",
    "\n",
    "def create_and_run_llm_loop(Incial_msg, logger, model=\"gpt-3.5-turbo\", num_iterations=10):\n",
    "    \n",
    "    agente = AssistantOpenAI(ARCLABKEY_OPENAI)\n",
    "\n",
    "    # Crea un asistente\n",
    "    asistente = agente.create_assistant(model=model, description=DESCRIPTION, instructions=INSTRUCTIONS, name=NAME, tools=TOOLS)\n",
    "    \n",
    "    # Crea un hilo\n",
    "    hilo = agente.create_thread()\n",
    "    \n",
    "    # Añade un mensaje inicial al hilo.\n",
    "    msg = agente.add_message(hilo.id, role=\"user\", content=Incial_msg)\n",
    "\n",
    "    # Bucle de aprendizaje del asistente.\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        logger.info(f\"Iteration: {i+1}\")\n",
    "        compiled = False\n",
    "\n",
    "        # Si en la presente iteración no se ha compilado el código, se pide al asistente corregir los errores antes de iterar.\n",
    "        while not compiled:\n",
    "            # Ejecuta el hilo\n",
    "            ejecucion = agente.run(hilo.id, asistente.id)\n",
    "            response = agente.get_run(ejecucion.id, hilo.id)\n",
    "\n",
    "            # Esperamos a que la respuesta esté lista.\n",
    "            while response.status not in [\"completed\", \"failed\", \"requires_action\"]:\n",
    "                logger.info(f\"Status: {ejecucion.status}\")\n",
    "                response = agente.get_run(ejecucion.id, hilo.id)\n",
    "                time.sleep(20)\n",
    "\n",
    "            # Descomponemos los elementos de la respuesta.\n",
    "            logger.info(f\"Status: {response.status}\")\n",
    "            if response.status == \"completed\":\n",
    "                return response\n",
    "            tool_call = response.required_action.submit_tool_outputs.tool_calls\n",
    "            print(f\"Tool call: {tool_call}\")\n",
    "\n",
    "            # Convierte el string a un diccionario\n",
    "            code_dict = json.loads(tool_call[0].function.arguments)\n",
    "            logger.info(f\"Arguments: {code_dict}\")\n",
    "            \n",
    "            # Obtén el código Python de la llamada a la herramienta\n",
    "            code = code_dict[\"code\"]\n",
    "            filename = code_dict[\"filename\"]  \n",
    "\n",
    "            logger.info(f\"\\nCodigo generado:\\n{code}\")\n",
    "\n",
    "            # Ejecuta el código Python\n",
    "            try:\n",
    "                store_code_in_file(code, filename)\n",
    "                time.sleep(1) # Pequeño retraso para que el sistema operativo pueda reflejar los cambios en el archivo\n",
    "                \n",
    "                # Hacemos commit de los cambios en el repositorio para analizar las modificaciones del agente.\n",
    "                commit_changes(r\"C:\\Users\\adria\\Repos\\TFM__LLM_landing_self-refinement\", f\"Corrección de errores de giro. Iteración {i+1}.\") \n",
    "                \n",
    "                importlib.reload(Action) # Recargamos el módulo de acciones para que se actualice con las modificaciones del agente.\n",
    "\n",
    "                # Ejecutamos el código generado.\n",
    "                logs = lunar_lander(n_games=1, display=True, recoder=True, video_filename=f\"video/iteration_{i+1}.mp4\")\n",
    "                \n",
    "                # Devolvemos la respuesta al asistente.\n",
    "                for call in tool_call:\n",
    "                    agente.devolver_respuesta(response.id, hilo.id, tool_outputs=[{\"tool_call_id\": call.id, \"output\": \"Run successful.\"}])            \n",
    "                compiled = True\n",
    "                logger.info(f\"Compilación exitosa.\")\n",
    "                \n",
    "                # Esperamos a que el agente esté listo para recibir mensajes y le añadimos el resultado de la iteración.\n",
    "                while response.status not in [\"completed\", \"failed\", \"expired\"]:\n",
    "                    logger.info(f\"Status: {response.status}\")\n",
    "                    response = agente.get_run(response.id, hilo.id)\n",
    "                    time.sleep(20)\n",
    "                \n",
    "                msg = f\"\"\"These are the logs generated by your last code: {logs}. Analyze the effect of the actions taken and compare it with previous logs to learn and generate a code that works better. Don't be afraid to make big changes, the total score must be over 200 points.\"\"\"\n",
    "                logger.info(msg)    \n",
    "                agente.add_message(hilo.id, role=\"user\", content=msg)\n",
    "            \n",
    "            # Alimentamos el asistente con el error generado en la ejecución del código.    \n",
    "            except Exception as e:\n",
    "                logger.exception(\"Error: %s\", e)\n",
    "                error_trace = traceback.format_exc()\n",
    "                for call in tool_call:\n",
    "                    agente.devolver_respuesta(response.id, hilo.id, tool_outputs=[{\"tool_call_id\": call.id, \"output\": \"ERROR.\"}]) \n",
    "                logger.error(f\"Error: {e}.\")\n",
    "                while response.status not in [\"completed\", \"failed\", \"expired\"]:\n",
    "                    logger.info(f\"Status: {response.status}\")\n",
    "                    response = agente.get_run(ejecucion.id, hilo.id)\n",
    "                    time.sleep(30)\n",
    "                msg = f\"The code generated has an error. Please, try again. Error: {e}. Trace: {error_trace}\"   \n",
    "                logger.error(msg)   \n",
    "                agente.add_message(hilo.id, role=\"assistant\", content=msg)\n",
    "    \n",
    "    agente.mostrar_mensajes(hilo.id)  \n",
    "    vaciar_agente(agente)\n",
    "    \n",
    "    logger.info(\"\\nEjecución finalizada.\\n\\n\")       \n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def vaciar_agente(agente):\n",
    "    for assistant in agente.assistants:\n",
    "        agente.delete_assistant(assistant)\n",
    "\n",
    "    for thread in agente.threads:\n",
    "        agente.delete_thread(thread)\n",
    "        \n",
    "    print(\"Asistente vaciado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configura_log(nombre_archivo):\n",
    "    \"\"\" \n",
    "    Configura el logger para que guarde los logs en un archivo y los muestre en la consola.\n",
    "    \n",
    "    Args:\n",
    "        nombre_archivo: str Nombre del archivo donde se guardarán los logs.\n",
    "        \n",
    "    Returns:\n",
    "        logger: logging.Logger Objeto logger configurado.\n",
    "    \"\"\"\n",
    "    # Crear la carpeta logs si no existe\n",
    "    if not os.path.exists('logs'):\n",
    "        os.makedirs('logs')\n",
    "    # Configura el logger\n",
    "    logging.basicConfig(filename=f'logs/{nombre_archivo}', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # Añade un StreamHandler para mostrar los logs en la consola\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución con el asistente\n",
    "\n",
    "Prompt inicial y mensajes del sistema para el asistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 18:44:06,308 - INFO - HTTP Request: POST https://api.openai.com/v1/assistants \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:44:06,541 - INFO - HTTP Request: POST https://api.openai.com/v1/threads \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:44:06,816 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/messages \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:44:06,963 - INFO - Iteration: 1\n",
      "2024-06-14 18:44:07,450 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:44:07,699 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs/run_GiypjBuhnt5RqjvJQCr3ylol \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:44:07,703 - INFO - Status: queued\n",
      "2024-06-14 18:44:07,963 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs/run_GiypjBuhnt5RqjvJQCr3ylol \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:44:27,971 - INFO - Status: queued\n",
      "2024-06-14 18:44:28,447 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs/run_GiypjBuhnt5RqjvJQCr3ylol \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:44:48,456 - INFO - Status: requires_action\n",
      "2024-06-14 18:44:48,459 - INFO - Arguments: {'code': 'import numpy as np\\n\\n\\nclass LunarLanderAgent:\\n    def __init__(self):\\n        # Define thresholds for decision making\\n        self.angle_threshold = 0.1  # radians\\n        self.velocity_threshold = 0.1  # velocity units\\n        self.position_threshold = 0.1  # position units\\n        self.previous_velocity = None\\n\\n    def act(self, observation):\\n        \\'\\'\\'\\n        The function that codifies the action to be taken in each instant of time.\\n\\n        Args:\\n            observation (numpy.array):\\n                \"description\": \"The state of the environment after the action is taken.\",\\n                \"positions\": {  \\n                    \"0\": \"X position\",\\n                    \"1\": \"Y position\",\\n                    \"2\": \"X velocity\",\\n                    \"3\": \"Y velocity\",\\n                    \"4\": \"Angle\",\\n                    \"5\": \"Angular velocity\",\\n                    \"6\": \"Left contact sensor\",\\n                    \"7\": \"Right contact sensor\"\\n                },\\n                \"min_values\": [-1.5, -1.5, -5.0, -5.0, -3.14, -5.0, 0, 0],\\n                \"max_values\": [1.5, 1.5, 5.0, 5.0, 3.14, 5.0, 1, 1]\\n\\n        Returns:\\n            Integer  : The action to be taken.\\n            \"options\": {\\n                    \\'0\\' : \"Switch off engines\",\\n                    \\'1\\' : \"Push left engine\",\\n                    \\'2\\' : \"Push both engines (upwards)\",\\n                    \\'3\\' : \"Push right engine\"\\n                }\\n        \\'\\'\\'\\n        x_pos, y_pos, x_vel, y_vel, angle, ang_vel, left_contact, right_contact = observation\\n\\n        # Stabilize the angle\\n        if abs(angle) > self.angle_threshold:\\n            if angle > 0:\\n                return 1  # Push left engine\\n            else:\\n                return 3  # Push right engine\\n\\n        # Reduce horizontal velocity\\n        if abs(x_vel) > self.velocity_threshold:\\n            if x_vel > 0:\\n                return 1  # Push left engine to move left\\n            else:\\n                return 3  # Push right engine to move right\\n\\n        # Control vertical velocity and position\\n        if y_vel < -self.velocity_threshold or y_pos < self.position_threshold:\\n            return 2  # Push both engines upwards\\n\\n        # If in contact with ground, do nothing (simulation completed)\\n        if left_contact == 1 or right_contact == 1:\\n            return 0  # Switch off engines\\n\\n        return 0  # Default action to switch off engines\\n\\n\\n# Instantiate the agent\\nagent = LunarLanderAgent()\\n\\n\\ndef act(observation):\\n    return agent.act(observation)\\n', 'filename': 'Action.py'}\n",
      "2024-06-14 18:44:48,459 - INFO - \n",
      "Codigo generado:\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class LunarLanderAgent:\n",
      "    def __init__(self):\n",
      "        # Define thresholds for decision making\n",
      "        self.angle_threshold = 0.1  # radians\n",
      "        self.velocity_threshold = 0.1  # velocity units\n",
      "        self.position_threshold = 0.1  # position units\n",
      "        self.previous_velocity = None\n",
      "\n",
      "    def act(self, observation):\n",
      "        '''\n",
      "        The function that codifies the action to be taken in each instant of time.\n",
      "\n",
      "        Args:\n",
      "            observation (numpy.array):\n",
      "                \"description\": \"The state of the environment after the action is taken.\",\n",
      "                \"positions\": {  \n",
      "                    \"0\": \"X position\",\n",
      "                    \"1\": \"Y position\",\n",
      "                    \"2\": \"X velocity\",\n",
      "                    \"3\": \"Y velocity\",\n",
      "                    \"4\": \"Angle\",\n",
      "                    \"5\": \"Angular velocity\",\n",
      "                    \"6\": \"Left contact sensor\",\n",
      "                    \"7\": \"Right contact sensor\"\n",
      "                },\n",
      "                \"min_values\": [-1.5, -1.5, -5.0, -5.0, -3.14, -5.0, 0, 0],\n",
      "                \"max_values\": [1.5, 1.5, 5.0, 5.0, 3.14, 5.0, 1, 1]\n",
      "\n",
      "        Returns:\n",
      "            Integer  : The action to be taken.\n",
      "            \"options\": {\n",
      "                    '0' : \"Switch off engines\",\n",
      "                    '1' : \"Push left engine\",\n",
      "                    '2' : \"Push both engines (upwards)\",\n",
      "                    '3' : \"Push right engine\"\n",
      "                }\n",
      "        '''\n",
      "        x_pos, y_pos, x_vel, y_vel, angle, ang_vel, left_contact, right_contact = observation\n",
      "\n",
      "        # Stabilize the angle\n",
      "        if abs(angle) > self.angle_threshold:\n",
      "            if angle > 0:\n",
      "                return 1  # Push left engine\n",
      "            else:\n",
      "                return 3  # Push right engine\n",
      "\n",
      "        # Reduce horizontal velocity\n",
      "        if abs(x_vel) > self.velocity_threshold:\n",
      "            if x_vel > 0:\n",
      "                return 1  # Push left engine to move left\n",
      "            else:\n",
      "                return 3  # Push right engine to move right\n",
      "\n",
      "        # Control vertical velocity and position\n",
      "        if y_vel < -self.velocity_threshold or y_pos < self.position_threshold:\n",
      "            return 2  # Push both engines upwards\n",
      "\n",
      "        # If in contact with ground, do nothing (simulation completed)\n",
      "        if left_contact == 1 or right_contact == 1:\n",
      "            return 0  # Switch off engines\n",
      "\n",
      "        return 0  # Default action to switch off engines\n",
      "\n",
      "\n",
      "# Instantiate the agent\n",
      "agent = LunarLanderAgent()\n",
      "\n",
      "\n",
      "def act(observation):\n",
      "    return agent.act(observation)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call: [RequiredActionFunctionToolCall(id='call_2RzHcgaWyJyEXKjrZtpZiYad', function=Function(arguments='{\"code\": \"import numpy as np\\\\n\\\\n\\\\nclass LunarLanderAgent:\\\\n    def __init__(self):\\\\n        # Define thresholds for decision making\\\\n        self.angle_threshold = 0.1  # radians\\\\n        self.velocity_threshold = 0.1  # velocity units\\\\n        self.position_threshold = 0.1  # position units\\\\n        self.previous_velocity = None\\\\n\\\\n    def act(self, observation):\\\\n        \\'\\'\\'\\\\n        The function that codifies the action to be taken in each instant of time.\\\\n\\\\n        Args:\\\\n            observation (numpy.array):\\\\n                \\\\\"description\\\\\": \\\\\"The state of the environment after the action is taken.\\\\\",\\\\n                \\\\\"positions\\\\\": {  \\\\n                    \\\\\"0\\\\\": \\\\\"X position\\\\\",\\\\n                    \\\\\"1\\\\\": \\\\\"Y position\\\\\",\\\\n                    \\\\\"2\\\\\": \\\\\"X velocity\\\\\",\\\\n                    \\\\\"3\\\\\": \\\\\"Y velocity\\\\\",\\\\n                    \\\\\"4\\\\\": \\\\\"Angle\\\\\",\\\\n                    \\\\\"5\\\\\": \\\\\"Angular velocity\\\\\",\\\\n                    \\\\\"6\\\\\": \\\\\"Left contact sensor\\\\\",\\\\n                    \\\\\"7\\\\\": \\\\\"Right contact sensor\\\\\"\\\\n                },\\\\n                \\\\\"min_values\\\\\": [-1.5, -1.5, -5.0, -5.0, -3.14, -5.0, 0, 0],\\\\n                \\\\\"max_values\\\\\": [1.5, 1.5, 5.0, 5.0, 3.14, 5.0, 1, 1]\\\\n\\\\n        Returns:\\\\n            Integer  : The action to be taken.\\\\n            \\\\\"options\\\\\": {\\\\n                    \\'0\\' : \\\\\"Switch off engines\\\\\",\\\\n                    \\'1\\' : \\\\\"Push left engine\\\\\",\\\\n                    \\'2\\' : \\\\\"Push both engines (upwards)\\\\\",\\\\n                    \\'3\\' : \\\\\"Push right engine\\\\\"\\\\n                }\\\\n        \\'\\'\\'\\\\n        x_pos, y_pos, x_vel, y_vel, angle, ang_vel, left_contact, right_contact = observation\\\\n\\\\n        # Stabilize the angle\\\\n        if abs(angle) > self.angle_threshold:\\\\n            if angle > 0:\\\\n                return 1  # Push left engine\\\\n            else:\\\\n                return 3  # Push right engine\\\\n\\\\n        # Reduce horizontal velocity\\\\n        if abs(x_vel) > self.velocity_threshold:\\\\n            if x_vel > 0:\\\\n                return 1  # Push left engine to move left\\\\n            else:\\\\n                return 3  # Push right engine to move right\\\\n\\\\n        # Control vertical velocity and position\\\\n        if y_vel < -self.velocity_threshold or y_pos < self.position_threshold:\\\\n            return 2  # Push both engines upwards\\\\n\\\\n        # If in contact with ground, do nothing (simulation completed)\\\\n        if left_contact == 1 or right_contact == 1:\\\\n            return 0  # Switch off engines\\\\n\\\\n        return 0  # Default action to switch off engines\\\\n\\\\n\\\\n# Instantiate the agent\\\\nagent = LunarLanderAgent()\\\\n\\\\n\\\\ndef act(observation):\\\\n    return agent.act(observation)\\\\n\", \"filename\": \"Action.py\"}', name='store_code_in_file'), type='function')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adria\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\adria\\Repos\\TFM__LLM_landing_self-refinement\\video\\iteration_1.mp4 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\adria\\Repos\\TFM__LLM_landing_self-refinement\\video\\iteration_1.mp4\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\adria\\Repos\\TFM__LLM_landing_self-refinement\\video\\iteration_1.mp4\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\adria\\Repos\\TFM__LLM_landing_self-refinement\\video\\iteration_1.mp4\\rl-video-episode-0.mp4\n",
      "Número de instantes: 73. Tamaño de logs: 37\n",
      "episode  1 score -612.950 avg score -8.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 18:44:54,148 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs/run_GiypjBuhnt5RqjvJQCr3ylol/submit_tool_outputs \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:44:54,152 - INFO - Compilación exitosa.\n",
      "2024-06-14 18:44:54,154 - INFO - Status: requires_action\n",
      "2024-06-14 18:44:54,454 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs/run_GiypjBuhnt5RqjvJQCr3ylol \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:45:14,457 - INFO - Status: in_progress\n",
      "2024-06-14 18:45:14,833 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs/run_GiypjBuhnt5RqjvJQCr3ylol \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:45:34,839 - INFO - These are the logs generated by your last code: {\"landing attempt\": 1, \"logs\": [\"{\\\"instant\\\": 0, \\\"action\\\": 1, \\\"current status\\\": [0.007, 1.417, 0.37, 0.131, -0.007, -0.044, 0.0, 0.0], \\\"score\\\": 1.387, \\\"completed\\\": false}\", \"{\\\"instant\\\": 2, \\\"action\\\": 1, \\\"current status\\\": [0.015, 1.421, 0.353, 0.077, -0.006, 0.026, 0.0, 0.0], \\\"score\\\": 1.473, \\\"completed\\\": false}\", \"{\\\"instant\\\": 4, \\\"action\\\": 1, \\\"current status\\\": [0.021, 1.423, 0.332, 0.024, 0.003, 0.11, 0.0, 0.0], \\\"score\\\": 1.038, \\\"completed\\\": false}\", \"{\\\"instant\\\": 6, \\\"action\\\": 1, \\\"current status\\\": [0.028, 1.422, 0.312, -0.03, 0.02, 0.189, 0.0, 0.0], \\\"score\\\": 0.039, \\\"completed\\\": false}\", \"{\\\"instant\\\": 8, \\\"action\\\": 1, \\\"current status\\\": [0.034, 1.419, 0.29, -0.084, 0.046, 0.277, 0.0, 0.0], \\\"score\\\": -0.807, \\\"completed\\\": false}\", \"{\\\"instant\\\": 10, \\\"action\\\": 1, \\\"current status\\\": [0.04, 1.414, 0.272, -0.138, 0.079, 0.349, 0.0, 0.0], \\\"score\\\": -1.802, \\\"completed\\\": false}\", \"{\\\"instant\\\": 12, \\\"action\\\": 1, \\\"current status\\\": [0.046, 1.406, 0.254, -0.192, 0.119, 0.422, 0.0, 0.0], \\\"score\\\": -2.398, \\\"completed\\\": false}\", \"{\\\"instant\\\": 14, \\\"action\\\": 1, \\\"current status\\\": [0.051, 1.395, 0.235, -0.248, 0.166, 0.496, 0.0, 0.0], \\\"score\\\": -3.172, \\\"completed\\\": false}\", \"{\\\"instant\\\": 16, \\\"action\\\": 1, \\\"current status\\\": [0.056, 1.382, 0.215, -0.304, 0.222, 0.578, 0.0, 0.0], \\\"score\\\": -3.857, \\\"completed\\\": false}\", \"{\\\"instant\\\": 18, \\\"action\\\": 1, \\\"current status\\\": [0.06, 1.367, 0.194, -0.362, 0.287, 0.67, 0.0, 0.0], \\\"score\\\": -4.56, \\\"completed\\\": false}\", \"{\\\"instant\\\": 20, \\\"action\\\": 1, \\\"current status\\\": [0.065, 1.349, 0.173, -0.42, 0.36, 0.758, 0.0, 0.0], \\\"score\\\": -5.145, \\\"completed\\\": false}\", \"{\\\"instant\\\": 22, \\\"action\\\": 1, \\\"current status\\\": [0.069, 1.329, 0.154, -0.478, 0.442, 0.839, 0.0, 0.0], \\\"score\\\": -5.65, \\\"completed\\\": false}\", \"{\\\"instant\\\": 24, \\\"action\\\": 1, \\\"current status\\\": [0.072, 1.306, 0.139, -0.538, 0.532, 0.916, 0.0, 0.0], \\\"score\\\": -6.215, \\\"completed\\\": false}\", \"{\\\"instant\\\": 26, \\\"action\\\": 1, \\\"current status\\\": [0.076, 1.28, 0.123, -0.599, 0.63, 1.001, 0.0, 0.0], \\\"score\\\": -6.534, \\\"completed\\\": false}\", \"{\\\"instant\\\": 28, \\\"action\\\": 1, \\\"current status\\\": [0.079, 1.252, 0.106, -0.663, 0.737, 1.099, 0.0, 0.0], \\\"score\\\": -7.093, \\\"completed\\\": false}\", \"{\\\"instant\\\": 30, \\\"action\\\": 1, \\\"current status\\\": [0.081, 1.222, 0.093, -0.726, 0.854, 1.188, 0.0, 0.0], \\\"score\\\": -7.423, \\\"completed\\\": false}\", \"{\\\"instant\\\": 32, \\\"action\\\": 1, \\\"current status\\\": [0.084, 1.188, 0.08, -0.791, 0.98, 1.289, 0.0, 0.0], \\\"score\\\": -8.016, \\\"completed\\\": false}\", \"{\\\"instant\\\": 34, \\\"action\\\": 1, \\\"current status\\\": [0.086, 1.152, 0.071, -0.857, 1.117, 1.386, 0.0, 0.0], \\\"score\\\": -8.276, \\\"completed\\\": false}\", \"{\\\"instant\\\": 36, \\\"action\\\": 1, \\\"current status\\\": [0.088, 1.113, 0.065, -0.922, 1.262, 1.476, 0.0, 0.0], \\\"score\\\": -8.699, \\\"completed\\\": false}\", \"{\\\"instant\\\": 38, \\\"action\\\": 1, \\\"current status\\\": [0.09, 1.072, 0.062, -0.988, 1.416, 1.568, 0.0, 0.0], \\\"score\\\": -9.095, \\\"completed\\\": false}\", \"{\\\"instant\\\": 40, \\\"action\\\": 1, \\\"current status\\\": [0.091, 1.028, 0.061, -1.055, 1.581, 1.673, 0.0, 0.0], \\\"score\\\": -9.416, \\\"completed\\\": false}\", \"{\\\"instant\\\": 42, \\\"action\\\": 1, \\\"current status\\\": [0.092, 0.981, 0.063, -1.121, 1.755, 1.763, 0.0, 0.0], \\\"score\\\": -9.857, \\\"completed\\\": false}\", \"{\\\"instant\\\": 44, \\\"action\\\": 1, \\\"current status\\\": [0.093, 0.931, 0.069, -1.184, 1.937, 1.842, 0.0, 0.0], \\\"score\\\": -9.938, \\\"completed\\\": false}\", \"{\\\"instant\\\": 46, \\\"action\\\": 1, \\\"current status\\\": [0.093, 0.878, 0.08, -1.251, 2.13, 1.946, 0.0, 0.0], \\\"score\\\": -10.358, \\\"completed\\\": false}\", \"{\\\"instant\\\": 48, \\\"action\\\": 1, \\\"current status\\\": [0.094, 0.822, 0.091, -1.313, 2.33, 2.025, 0.0, 0.0], \\\"score\\\": -10.513, \\\"completed\\\": false}\", \"{\\\"instant\\\": 50, \\\"action\\\": 1, \\\"current status\\\": [0.094, 0.763, 0.108, -1.374, 2.539, 2.115, 0.0, 0.0], \\\"score\\\": -10.843, \\\"completed\\\": false}\", \"{\\\"instant\\\": 52, \\\"action\\\": 1, \\\"current status\\\": [0.095, 0.701, 0.13, -1.434, 2.757, 2.209, 0.0, 0.0], \\\"score\\\": -10.925, \\\"completed\\\": false}\", \"{\\\"instant\\\": 54, \\\"action\\\": 1, \\\"current status\\\": [0.096, 0.635, 0.148, -1.49, 2.984, 2.284, 0.0, 0.0], \\\"score\\\": -11.009, \\\"completed\\\": false}\", \"{\\\"instant\\\": 56, \\\"action\\\": 1, \\\"current status\\\": [0.097, 0.566, 0.169, -1.543, 3.218, 2.359, 0.0, 0.0], \\\"score\\\": -11.137, \\\"completed\\\": false}\", \"{\\\"instant\\\": 58, \\\"action\\\": 1, \\\"current status\\\": [0.098, 0.495, 0.192, -1.592, 3.461, 2.45, 0.0, 0.0], \\\"score\\\": -11.209, \\\"completed\\\": false}\", \"{\\\"instant\\\": 60, \\\"action\\\": 1, \\\"current status\\\": [0.1, 0.42, 0.209, -1.637, 3.711, 2.529, 0.0, 0.0], \\\"score\\\": -11.288, \\\"completed\\\": false}\", \"{\\\"instant\\\": 62, \\\"action\\\": 1, \\\"current status\\\": [0.102, 0.342, 0.223, -1.681, 3.971, 2.608, 0.0, 0.0], \\\"score\\\": -11.594, \\\"completed\\\": false}\", \"{\\\"instant\\\": 64, \\\"action\\\": 1, \\\"current status\\\": [0.105, 0.262, 0.23, -1.722, 4.237, 2.685, 0.0, 0.0], \\\"score\\\": -11.824, \\\"completed\\\": false}\", \"{\\\"instant\\\": 66, \\\"action\\\": 1, \\\"current status\\\": [0.109, 0.179, 0.234, -1.762, 4.512, 2.772, 0.0, 0.0], \\\"score\\\": -12.313, \\\"completed\\\": false}\", \"{\\\"instant\\\": 68, \\\"action\\\": 1, \\\"current status\\\": [0.113, 0.094, 0.231, -1.802, 4.796, 2.863, 0.0, 0.0], \\\"score\\\": -13.326, \\\"completed\\\": false}\", \"{\\\"instant\\\": 70, \\\"action\\\": 1, \\\"current status\\\": [0.118, 0.008, 0.227, -1.826, 5.08, 2.809, 1.0, 0.0], \\\"score\\\": -13.609, \\\"completed\\\": false}\", \"{\\\"instant\\\": 71, \\\"action\\\": 1, \\\"current status\\\": [0.122, -0.003, 0.469, -0.555, 4.795, -5.984, 1.0, 0.0], \\\"score\\\": -100, \\\"completed\\\": true}\"], \"total score\": -612.9504793446796}. Analyze the effect of the actions taken and compare it with previous logs to learn and generate a code that works better. Don't be afraid to make big changes, the total score must be over 200 points.\n",
      "2024-06-14 18:45:35,311 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/messages \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:45:35,315 - INFO - Iteration: 2\n",
      "2024-06-14 18:45:35,825 - INFO - HTTP Request: POST https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:45:36,131 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs/run_82ISv7jWBBFWMSywoGcCkGtu \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:45:36,134 - INFO - Status: queued\n",
      "2024-06-14 18:45:36,438 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs/run_82ISv7jWBBFWMSywoGcCkGtu \"HTTP/1.1 200 OK\"\n",
      "2024-06-14 18:45:56,442 - INFO - Status: queued\n",
      "2024-06-14 18:45:56,714 - INFO - HTTP Request: GET https://api.openai.com/v1/threads/thread_AAYIKde8Do4hZYPOZnH0Jxse/runs/run_82ISv7jWBBFWMSywoGcCkGtu \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m initial_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the record of an example of a successful landing in this environment, but under other conditions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuccess_logs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You have to be able to learn from it to land successfully with any other conditions. This is the code of the initial function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_initial_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and these are the execution logs of one landing attempt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_logs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.Take a deep breath and reason step-by-step. After reasoning analyze the results, learn and make better code.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m logger \u001b[38;5;241m=\u001b[39m configura_log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpacecraft_4o_betterprompt.log\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m response \u001b[38;5;241m=\u001b[39m create_and_run_llm_loop(initial_msg, logger, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[12], line 35\u001b[0m, in \u001b[0;36mcreate_and_run_llm_loop\u001b[1;34m(Incial_msg, logger, model, num_iterations)\u001b[0m\n\u001b[0;32m     33\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mejecucion\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m     response \u001b[38;5;241m=\u001b[39m agente\u001b[38;5;241m.\u001b[39mget_run(ejecucion\u001b[38;5;241m.\u001b[39mid, hilo\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m---> 35\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Descomponemos los elementos de la respuesta.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DESCRIPTION = \"You are an expert programer in Pyhton. Your specialty is to generate the code responsible for making decisions about actions to be taken in various spacecraft landing environments.The objective is to land the spacecraft within a target zone in the shortest possible time and very gently. A scoring system is used to evaluate the landings, which must be maximized.\"\n",
    "INSTRUCTIONS = f\"\"\"Your task is:\n",
    "1. Analyze and reason about the logs received in the last landing attempts.\n",
    "2. Your goal is to be able to make the correct decision based on what you have learned from the results of previous iterations. You must code the decision making based on your reasoning in a Python function.\n",
    "3. IMPORTANT. Use the following tips in your reasoning to achieve a successful landing:\n",
    "    - First you have to stabilize the falling ship (angle and location), keep falling under control and then land gently at the end.\n",
    "    - It is mandatory to use all the elements of the array of observations received by parameter in your code when deciding what action to take at any given moment. Both position and velocities must be taken into account to know how the ship is doing and towards which states it is heading. All of these must be considered to achieve stability.\n",
    "    - Learn how actions taken affect the future states of the spacecraft in the logs of past events so that you can take this into account when developing code to reach the landing zone.\n",
    "    - The landing zone is in the central area of the x-axis\n",
    "4. You should analyze the performance that appear in the logs of the code you have generated. You should improve the code generated in the 'act' function in the last iteration without fear of making major changes, seeking to maximize the score received and generate a higher quality code.\n",
    "5. Save the code of the act function in the file 'Action.py' using store_code_in_file function.\n",
    "\"\"\"\n",
    "# 6. Improve your results and correct also any programming error you may have generated in your last code if they exist.\n",
    "NAME = \"Spacecraft Landing Master\"\n",
    "\n",
    "initial_msg = f\"This is the record of an example of a successful landing in this environment, but under other conditions: {success_logs}. You have to be able to learn from it to land successfully with any other conditions. This is the code of the initial function: {agent_initial_code} and these are the execution logs of one landing attempt: {initial_logs}.Take a deep breath and reason step-by-step. After reasoning analyze the results, learn and make better code.\"\n",
    "\n",
    "logger = configura_log('Spacecraft_4o_betterprompt.log')\n",
    "response = create_and_run_llm_loop(initial_msg, logger, model=\"gpt-4o\", num_iterations=5)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Incluir guardado de vídeo.\n",
    "* Mejorar el prompt inicial.\n",
    "* Incluir ejemplos positivos.\n",
    "* Incluir cambios en el código con GIT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
